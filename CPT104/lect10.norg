* 10. I/O Systems — 详细笔记
** 10.0 引言
   - **I/O 管理**是 OS 核心组成：≈ CPU 调度、内存管理 同等重要  
   - 难点  
   - 设备类型众多，速度 / 接口 / 语义各异  
   - 新硬件不断出现，需要可扩展的驱动框架  
   - 性能瓶颈常出现在 I/O（“磁盘墙”“PCIe 墙”）  

** 10.1 设备分类
*** 10.1.1 按“人/机”可读性  
    - *Human‑readable*（人机交互）  
    - 键盘、鼠标、触控屏、音频输出、打印机、显示器  
    - *Machine‑readable*（机器‑机器）  
    - 传感器：温湿度、陀螺仪  
    - 控制器：磁盘、网卡、GPU、USB‑Hub  
*** 10.1.2 按数据传输粒度  
    - *Character‑oriented*：逐字节 / 字符流  
    - 键盘、串口、网络接口  
    - *Block‑oriented*：固定块（通常 512 B~4 KiB）  
    - 磁盘、SSD、Tape（块模拟）、光驱  
*** 10.1.3 按访问方式  
    - *Sequential*：只能顺序访问；Tape、某些摄像头流  
    - *Random (Direct)*：可 O(1) 定位；磁盘、SSD、RAM Disk  

** 10.2 I/O 硬件抽象
*** 10.2.1 端口（Port）  
    - 一个可寻址寄存器组，用于 CPU ↔ 设备控制器通信  
    - x86：IN/OUT 指令访问 I/O 端口空间（0‑0xFFFF）  
*** 10.2.2 总线（Bus）  
    - **系统总线**：CPU <-> 内存 <-> 高速 I/O（SoC 内 AMBA、AMD GNB）  
    - **扩展总线**：PCIe、旧 PCI、ISA；USB / Thunderbolt（外部）  
    - Daisy‑chain 或多点并联，支持仲裁 + 热插拔  
*** 10.2.3 控制器 / Host Adapter  
    - 处理端口上的命令，将其转换为总线协议，再驱动设备  
    - 可板载（SoC USB3 Host）也可独立卡（NVMe HBA）  
    - 典型寄存器：  
    | 名称          | 方向 | 功能                              |
    | data‑in       | R    | 读取来自设备的数据                |
    | data‑out      | W    | 写数据到设备缓冲区                |
    | status        | R    | 位标志：BUSY、READY、ERR…         |
    | control / cmd | W    | 启动操作、复位、DMA 地址、长度等   |
*** 10.2.4 设备寻址方式  
    - *Port‑mapped I/O (PMIO)*  
    - CPU 使用专门的 I/O 指令访问；硬件译码不同地址空间  
    - 优点：内核与用户页表不用特别隔离；缺点：指令受限，难做 DMA 映射  
    - *Memory‑mapped I/O (MMIO)*  
    - 固定物理地址范围映射到设备寄存器 / FIFO  
    - 普通 LD/ST 指令即可访问 → 低开销、大吞吐（GPU、NIC）  
    - 需要页表标记 UNCACHEABLE、防止用户态越权  

** 10.3 通信技术
*** 10.3.1 轮询 (Polling, Programmed I/O)  
    - CPU 循环读取 status 寄存器 BUSY 位  
    - 简单、无中断开销；当设备慢时浪费 CPU  
*** 10.3.2 中断驱动 I/O  
    - 设备就绪→控制器置中断线→CPU 响应进入 ISR  
    - *NMI*（不可屏蔽）/ *IRQ*（可屏蔽）  
    - ISR 工作：  
      1. 保存上下文，识别来源  
      2. 读取数据或清状态  
      3. 唤醒阻塞进程 / 触发软中断  
    - 优点：CPU 利用率高；缺点：高 IOPS 场景下中断风暴  
*** 10.3.3 DMA — 直接内存访问  
    - 控制器带 DMA 引擎，可在**不经过 CPU**情况下→ Mem↔Dev  
    - DMA 描述符：物理地址 + 长度 + 标志  
    - 结束后产生中断；减轻 CPU 拷贝开销  
    - 高级：Scatter/Gather、IOMMU 保护、NVMe PRP/SGL  

** 10.4 系统架构示例（图解）
   /—— CPU
   │<==========系统总线/SoC互联==========>│
   Disk Ctlr──磁盘
   USB Host──鼠/键/打印机
   GPU/Display Ctlr──显示器
   ▲
   └── 内存控制器（共享 DRAM）
   - 多控制器挂一根高速总线，统一访问内存  
   - 每个控制器再以专有总线 (SATA/SAS/USB) 连接多个设备  
   - I/O 请求路径：用户态 → 系统调用 → 驱动 → 控制器寄存器 → DMA → 中断 → 应用完成  

** 10.5 设备驱动在 OS 中的位置
   - *用户态库*：libusb / GLFW（少数）  
   - *内核态驱动*：分层  
   - *上半部*（top half）：快速处理中断、调度 work queue  
   - *下半部*（bottom half）：执行阻塞任务 (DMA map/unmap, copy)  
   - 驱动向 VFS / TTY / NET 子系统注册 file_operations, net_device_ops …  
** 10.6 性能与瓶颈
   - IOPS vs 吞吐量：随机 4 K vs 顺序 1 M  
   - 中断聚合(IRQ coalescing)、MSI‑X、多队列 (NVMe, virtio)  
   - Zero‑copy：splice/sendfile、DMA‑bounce‑buffer 消除  
   - NUMA 亲和：将设备中断、DMA 绑定到同 NUMA 节点 CPU + 内存  
** 10.7 现代趋势
   - 高速互连：PCIe 5.0 / CXL；GPU RDMA  
   - SmartNIC / DPU：控制器内嵌多核 CPU 直接跑协议栈  
   - 用户态驱动框架：DPDK、SPDK、io_uring (offload 到内核线程)  

** 10.2 Application‑Level I/O 接口详解
   描述用户程序如何通过 **I/O 软件层** 与多样的 **I/O 硬件层** 交互。

** 10.2.1 I/O 软件栈（参见右图）
  ┌─ Application (用户进程)
  │
  ├─ User‑level I/O library · stdio / glibc / Win32 API
  │  - 缓冲 / 格式化 / scatter‑gather readv writev
  │
  ├─ Kernel I/O 子系统 (VFS, socket, TTY, block I/O)
  │  - 系统调用层：open, read, write, ioctl, mmap …
  │  - 统一文件描述符抽象；调度、缓存、权限检查
  │
  ├─ Device Drivers
  │  - 针对具体控制器实现 register read/write、DMA 设置
  │
  └─ Interrupt Handlers
     - 在中断上下文快速完成任务：确认 IRQ、唤醒进程
     - 将耗时工作推迟到 bottom‑half / work‑queue

** 10.2.2 设备语义差异抽象
*** Block Device
    - 随机寻址（seek+read/write）
    - 支持 RAW I/O（裸块访问）与 Buffered I/O（文件系统）
*** Character Device
    - 字节流：get(), put()
    - 典型：键盘串口 TTY、音频
*** 网络设备 (socket)
    - 面向报文 / 字节流；协议栈处理分段、重传
*** Clock / Timer
    - ioctl 或 timerfd 设置周期 / 单次定时器
    - PIT / HPET / TSC 提供高分辨率时间源

** 10.2.3 访问模式
   | 模式            | 行为                                             | 适用场景 |
   | Blocking        | 线程进入等待队列，I/O 完成后被唤醒               | 简单 API、低并发 |
   | Non‑blocking    | 立即返回 EAGAIN；配合 poll/epoll 轮询             | GUI、事件驱动 |
   | Asynchronous    | 提交后立即返回；完成通过回调 / signal / io_uring  | 高吞吐网络、存储 |
   | Vectored I/O    | readv/writev：一次系统调用读写多缓冲区            | scatter‑gather DMA |

** 10.2.4 DMA 工作流程（左图 1‑6）
   1. 驱动将 **缓冲区首地址 X** + **传输计数 C** 写入 DMA 控制器寄存器  
   2. 驱动下达 *READ disk → mem* 命令给磁盘控制器  
   3. 磁盘控制器发起 PCIe 或 SATA DMA 请求  
   4. 每收到一字节 / 一块数据即写入 DMA FIFO  
   5. **DMA 控制器** 依据总线掌握内存，将数据直写 *memory[X…]*，同时 ++addr, --C  
   6. 计数 C=0 → DMA 触发 IRQ；ISR 更新 I/O 状态并唤醒等待进程  
   > 优点：CPU 只在开始 / 结束介入，省去搬运开销；可与 Cache 一致性协议协同 (PCIe Snoop, IOMMU)

** 10.2.5 设备寻址实现
*** Port‑mapped I/O (PMIO)
    - 独立地址空间，x86 使用 IN/OUT 指令  
    - 不会污染 Cache，但一次只能传 1/2/4 B  
*** Memory‑mapped I/O (MMIO)
    - 设备寄存器作为“不可缓存”页直接映射到物理地址  
    - 允许普通 LOAD/STORE 和 DMA；GPU / NIC 必备  
    - 需页表标记 UC / WC，防止投机写入

** 10.2.6 高性能 I/O 优化
   - 中断合并 (IRQ Coalescing) / MSI‑X 多队列，减少上下文切换  
   - Zero‑copy：mmap+writev、splice、AF_XDP、io_uring MSG_ZEROCOPY  
   - NUMA 亲和：映射 DMA 缓冲与处理线程到同节点  
   - 用户态快速路径：DPDK (轮询)+hugepage、SPDK 直控 NVMe


** 10.3 Kernel‑Level I/O 子系统
   位于 **内核态**，向上为用户程序 / 系统调用提供统一 I/O 服务，向下与具体驱动、硬件交互。

** 10.3.1 统一接口（Uniform Interface）
   - 内核对外只暴露 open/read/write/ioctl 等固定系统调用。  
   - 各类设备驱动通过 ​`file_operations`​/​`struct dev_ops`​ 等回调注册 —— *多驱动一接口*。  
   - 利于可移植性：新设备→写驱动→挂上统一接口即可。

** 10.3.2 请求调度（I/O Scheduler）
   - 每个设备维护 *请求队列*，将用户 I/O 合并、重排。  
   - 典型算法  
   - **FIFO**：低负载简易  
   - **SSTF / CFQ / BFQ**：按最短寻道或按进程权重公平  
   - **Deadline**：给每请求打时限，避免饥饿  
   - 目标：吞吐 & 公平折中，减少机械寻道或 PCIe 链路争用。

** 10.3.3 缓冲与缓存（Buffer vs Cache）
   | 概念 | 目的 | 生命周期 |
   | Buffer | **传输瞬时缓冲区**：解决生产/消费速率差 | 传输结束即丢 |
   | Cache  | **加速重用**：保留热门块以减少后续访问延迟 | 基于替换策略 (LRU, ARC) |
   - 内核页缓存 (page cache) 既可充当读缓存，也充当 write‑back 缓冲。

*** 单缓冲 & 双缓冲（图示）
    - *User‑space buffering*：库函数 `fread` 维护用户缓冲，系统一次拷⻉。  
    - *Kernel‑space buffering*：无用户缓冲；系统直接 copy_to_user / copy_from_user。  
    - *Double buffering*：内核交替使用 Buffer1 / Buffer2  
    - **优点**：I/O 设备写 Buffer1 的同时，CPU 可处理 Buffer2 → *流水线*  
    - 常见于音频播放、网络收包环形缓冲 NAPI。

** 10.3.4 Spooling（SPOOL = Simultaneous Peripheral Operations On‑Line）
   - 针对一次只能服务单流的设备（打印机、批量刻录）  
   - 内核将作业写入后台队列目录（/var/spool/*），后台 daemon 顺序送入设备 → 支持多用户并发提交。

** 10.3.5 设备/内存保护
   - I/O 指令、MMIO 地址均标记为 **特权**，只能在内核态执行。  
   - `request_region()` / `ioremap_nocache()` 申请并锁定端口 / 物理页，防止其他驱动踩踏。  
   - 通过 **互斥锁 / spinlock / semaphore** 保证独占（例如 *open‑once* 字符设备）。

** 10.3.6 错误处理
   - 硬件返回状态字 → 驱动翻译为 `-EIO / -ENOSPC / -ETIMEDOUT …`  
   - 重试策略：瞬时错误（CRC、buffer overflow）→ 指数回退重传；永久错误（介质坏块）→ remap / 上报 S.M.A.R.T。  
   - 用户空间通过 `errno` 或 `aio_error()` 获取错误码。

** 10.3.7 DMA & 零拷贝协同
   1. 进程 `write()` → 驱动将用户页锁定，提交 (phys_addr, len) 给 DMA 控制器  
   2. 设备 → DMA → 内存（或反向）零拷贝  
   3. DMA 结束触发 **中断**，ISR 更新队列 head，唤醒等待进程 / 完成 *io_uring CQE*。
   若设备与 CPU 不在同一 NUMA node，应使用 bounce buffer 或启用 IOMMU 以保证地址可达。


** 10.4 Device Drivers
   最底层直接与硬件控制器交互的内核模块；把 **具体设备差异** 隐藏在统一 I/O 接口之后，使内核上层与应用程序“看见”的都是 open/read/write/ ioctl 等通用语义。

** 10.4.1 角色与职责
   1. **接收 I/O 请求**  
   - 上层（VFS、块层、net stack、chrdev core…）通过 *file_operations*、blk_mq、net_device 等接口向驱动提交 bio / skb / iov 等请求结构。
     2. **设备初始化与配置**  
   - 上电、复位，设置寄存器、DMA 描述符；建立中断向量（`request_irq()`）、创建 sysfs 节点。
     3. **执行 I/O**  
   - 发命令到控制器（写寄存器 / MMIO），启动 DMA 或 PIO 循环。  
   - 维护内部队列与状态机（busy / idle）。
     4. **中断 / 轮询处理**  
   - **ISR** 读取状态字、清中断、提取或填充数据缓冲，唤醒等待队列 (`wake_up`) 或完成回调 (`complete`, `io_complete`).  
   - 对高吞吐网卡使用 NAPI 软轮询或 MSI‑X 多队列分发。
     5. **错误检测与恢复**  
   - 超时、CRC、硬件故障 → 重试 / 重置通道；严重错误上报内核 (`dev_err`  + `blk_queue_error()` …)。
     6. **资源管理**  
   - 申请 / 释放 I/O 端口、MMIO 区域、DMA 缓冲、IRQ line；在 `probe()`‑`remove()` 生命周期钩子中完成。

** 10.4.2 驱动类型
   - **字符设备驱动**：tty、串口、传感器；接口面向字节流。  
   - **块设备驱动**：磁盘、SSD、NBD；提供随机 512B/4K 块访问；挂接至 VFS。  
   - **网络设备驱动**：NIC、Wi‑Fi；收发 `sk_buff` 并与内核网络协议栈对接。  
   - **混合 / 控制驱动**：GPU 驱动提供 DMA‑Buf、/dev/dri 以及 ioctl 控制面。  
   - **虚拟驱动**：loop、ramdisk、vhost‑net、FUSE、virtio 系列等。

** 10.4.3 标准框架
   | OS | 核心抽象 | 典型注册 |
   | Linux | `platform_driver`, `pci_driver`, `usb_driver` | `module_init()`‑`probe()`‑`remove()` |
   | Windows | WDM / KMDF / UMDF | `DriverEntry` + IRP 处理例程 |
   | BSD | bus_space + `device_t` | `device_probe`, `device_attach` |

** 10.4.4 同步与并发
   - **上下文**：进程上下文、软中断、硬中断。  
   - 关键资源使用 `spin_lock_irqsave`（中断安全）或 `mutex`（可睡眠）。  
   - 多队列块驱动 blk‑mq & tag‑set 支持 per‑CPU hw queue，减少锁争用。  

** 10.4.5 调试与维护
   - `dmesg`, `dev_dbg`, `traceevents`, `ftrace`, `perf`, `dynamic_debug`。  
   - `sysfs` 属性暴露运行时统计与调节开关。  
   - 版本化 ABI (ioctl, ethtool, drm uAPI) 避免破坏用户态兼容性。  

   > **小结**：设备驱动=“硬件微内核”，负责把物理设备抽象成统一的文件 / 网络 / 块接口；其质量直接决定操作系统对设备的性能与稳定性。


** 10.5 Interrupt Handling
   当设备控制器完成一次 I/O（或出现错误、状态变化）时，通过 *中断线* 向 CPU 发送信号 → 触发 **Interrupt Service Routine (ISR)**。  
   **目标**：让设备异步工作而 CPU 只在“必须介入”时被打断，既保证及时响应也避免忙等浪费。

*** 10.5.1 硬件到软件的完整流程**  
    1. 设备控制器把中断线拉高 ➜ I/O APIC / GIC 等 *中断控制器* 把该中断号送入 CPU。  
    2. CPU 执行完当前指令后检测到中断，**自动** 保存少量寄存器 + EFLAGS ➜ 跳转到 *中断向量表* 所记录的入口。  
    3. **ISR 顶半部**（hard IRQ）  
    - 关本地中断，读取设备状态寄存器，清除中断源；  
    - 将数据包、I/O 完成标志、错误信息等放入环形缓冲或队列；  
    - 排队或唤醒 **底半部**（softirq / tasklet / workqueue）后尽快返回。  
      4. **底半部** 在可抢占上下文完成重量级工作：拷贝数据、唤醒等待队列、submit 新的 I/O、更新统计。  
      5. 最后通过 `iret` 恢复被打断进程或调度到更高优先级任务。  

*** 10.5.2 ISR 的关键设计点**  
    - **最小化执行时间**：硬中断内禁止睡眠、禁止抢占；必须尽量短 → 把耗时逻辑挪到 softirq/tasklet/workqueue。  
    - **可重入与并发**：同一 IRQ 可能多核并发触发；共享中断需检查 `dev_id` 消除“伪中断”。  
    - **优先级与屏蔽**：有级联中断控制器时可动态 Mask / Unmask；实时系统关注 *中断延迟* 与 *抖动*。  
    - **中断风暴削减**：MSI/MSI‑X 多队列、NAPI 轮询、限速触发阈值（coalescing）。  

    **10.5.3 STREAMS 架构示意（图所示）**  
    - SysV STREAMS 采用 *分层队列*：  
      `user process ↔ stream head ↔ {read|write queue}* ↔ driver end ↔ device`  
    - *stream head* 提供 read()/write()/ioctl() 接口；中间可插拔协议或滤镜模块；driver end 负责与物理设备和 ISR 对接。  

** 10.7 I/O 性能优化
*** 1 减少上下文切换
    - 合并小 I/O → 大事务；使用 `io_uring` / AIO、`sendfile()` 零拷贝。  
**** 2 减少数据拷贝**  
     - DMA 直达用户缓冲 (uio / DPDK)、`mmap()`、内核零拷贝 (splice / AF_XDP)。  
**** 3 减少中断开销**  
     - 批量传输 + 中断合并 (coalescing)；高负荷下切换为轮询 (NAPI)。  
**** 4 启用 DMA**  
     - 让 DMA 控制器负责搬运，CPU 只做设置 + 完成中断处理。  
**** 5 使用更智能的硬件**  
     - NIC with off‑load (checksum, TSO, RSS)、NVMe Submission/Completion queue、智能存储控制器。  
**** 6 系统级平衡**  
     - 评估 CPU 核数、内存带宽、PCIe 通道、设备并发队列，避免单点瓶颈；NUMA 绑定提高本地内存/设备亲和性。  
     _提示_：驱动 + 中断 + DMA + 缓存策略共同决定 I/O 路径性能；任何环节的多一次复制、多一次上下文切换都可能成为瓶颈。


